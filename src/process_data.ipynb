{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "080f0a18-5411-47da-80ae-ec1a5059a10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw training data loaded successfully.\n",
      "Pipeline successfully transformed data.\n",
      "root\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+--------+----------------------------------------------------+\n",
      "|Survived|features                                            |\n",
      "+--------+----------------------------------------------------+\n",
      "|0       |(12,[0,1,2,4,7,9],[22.0,7.25,2.0,1.0,1.0,1.0])      |\n",
      "|1       |(12,[0,1,2,5,8,10],[38.0,71.2833,2.0,1.0,1.0,1.0])  |\n",
      "|1       |[26.0,7.925,1.0,1.0,1.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0]|\n",
      "|1       |(12,[0,1,2,5,8,9],[35.0,53.1,2.0,1.0,1.0,1.0])      |\n",
      "|0       |[35.0,8.05,1.0,1.0,1.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0] |\n",
      "+--------+----------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "Processed data saved to 'data/processed/titanic_ml_ready'.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, when, mean, mode\n",
    "\n",
    "def create_preprocessing_pipeline(df):\n",
    "    \"\"\"\n",
    "    Creates a PySpark ML Pipeline for preprocessing the Titanic dataset.\n",
    "    \n",
    "    Args:\n",
    "        df: The raw Spark DataFrame.\n",
    "        \n",
    "    Returns:\n",
    "        A PySpark ML Pipeline model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Handle Missing Values\n",
    "    # Impute missing 'Age' with the mean.\n",
    "    mean_age = df.agg(mean(col('Age'))).collect()[0][0]\n",
    "    \n",
    "    # Impute missing 'Embarked' with the mode.\n",
    "    mode_embarked = df.groupBy('Embarked').count().orderBy(col('count').desc()).first()[0]\n",
    "    \n",
    "    # Impute missing 'Fare' with the mean.\n",
    "    mean_fare = df.agg(mean(col('Fare'))).collect()[0][0]\n",
    "\n",
    "    # Use a custom transformer or direct operations for imputation.\n",
    "    # For this example, we'll apply these operations before the pipeline.\n",
    "    # In a real-world scenario, you might use a custom Transformer.\n",
    "    df = df.fillna(mean_age, subset=['Age'])\n",
    "    df = df.fillna(mode_embarked, subset=['Embarked'])\n",
    "    df = df.fillna(mean_fare, subset=['Fare'])\n",
    "\n",
    "    # 2. Feature Engineering\n",
    "    df = df.withColumn('FamilySize', col('SibSp') + col('Parch') + 1)\n",
    "    df = df.withColumn('IsAlone', when(col('FamilySize') == 1, 1).otherwise(0))\n",
    "\n",
    "    # 3. Define Categorical and Numerical Columns\n",
    "    categorical_cols = ['Pclass', 'Sex', 'Embarked']\n",
    "    numerical_cols = ['Age', 'Fare', 'FamilySize', 'IsAlone']\n",
    "\n",
    "    # 4. Create Pipeline Stages\n",
    "    \n",
    "    # Index all categorical columns\n",
    "    indexers = [\n",
    "        StringIndexer(inputCol=c, outputCol=f\"{c}_index\", handleInvalid='keep')\n",
    "        for c in categorical_cols\n",
    "    ]\n",
    "    \n",
    "    # One-hot encode the indexed columns\n",
    "    encoders = [\n",
    "        OneHotEncoder(inputCol=f\"{c}_index\", outputCol=f\"{c}_vector\")\n",
    "        for c in categorical_cols\n",
    "    ]\n",
    "    \n",
    "    # Final list of all features to be assembled\n",
    "    feature_columns = numerical_cols + [f\"{c}_vector\" for c in categorical_cols]\n",
    "\n",
    "    # Assemble all features into a single vector\n",
    "    assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "    # 5. Create the Pipeline\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "    \n",
    "    return pipeline, df\n",
    "\n",
    "\n",
    "def process_and_save_data():\n",
    "    \"\"\"\n",
    "    Main function to run the data processing pipeline.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"TitanicMLPreprocessing\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    try:\n",
    "        df = spark.read.csv('../data/raw/train.csv', header=True, inferSchema=True)\n",
    "        print(\"Raw training data loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        spark.stop()\n",
    "        return\n",
    "\n",
    "    # Create and fit the pipeline\n",
    "    pipeline, df_imputed = create_preprocessing_pipeline(df)\n",
    "    pipeline_model = pipeline.fit(df_imputed)\n",
    "    \n",
    "    # Transform the data using the fitted pipeline model\n",
    "    processed_df = pipeline_model.transform(df_imputed)\n",
    "\n",
    "    # Select the final columns: 'Survived' and the 'features' vector\n",
    "    final_df = processed_df.select('Survived', 'features')\n",
    "    \n",
    "    print(\"Pipeline successfully transformed data.\")\n",
    "    final_df.printSchema()\n",
    "    final_df.show(5, truncate=False)\n",
    "\n",
    "    try:\n",
    "        # Save the processed DataFrame. This can now be used for model training.\n",
    "        final_df.write.mode('overwrite').parquet('../data/processed/titanic_ml_ready')\n",
    "        print(\"Processed data saved to 'data/processed/titanic_ml_ready'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving processed data: {e}\")\n",
    "\n",
    "    spark.stop()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_and_save_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3faeec-f9d9-4f51-801f-f5922f6a1934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
